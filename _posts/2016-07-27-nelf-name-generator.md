---
layout: post
title: Nelf Name Generator
---

I have taken a look into [LSTM-RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) for the works of ICT&Cambricon. I found the LSTM language model is kind of attractive to me.

I have built a name generator for Nelf female. Export all Nelf female NPC names from a private server database(Arkcore NG, 4.3.2), and delete last names and titles manually, just save all first names. There are 498 first names at total(maybe have few duplicates).

Run a python script to generate a random sequence of 200,000 names as training data. Build a mini LSTM network with 2 layers, each has 1000 neurons, and a 50% drop rate after each layer.

<div><button class="collapsible">Collapsed</button><div class="collapsible-content">
<pre><code>
name: "lstm_language_model"
layer {
  name: "data"
  type: "HDF5Data"
  top: "cont_sentence"
  top: "input_sentence"
  top: "target_sentence"
  include { phase: TRAIN }
  hdf5_data_param {
    source: "hdf5_list.txt"
    batch_size: 200
  }
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "cont_sentence"
  top: "input_sentence"
  top: "target_sentence"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "hdf5_list.txt"
    batch_size: 1
  }
}
layer {
  name: "lstm1"
  type: "LSTM"
  bottom: "input_sentence"
  bottom: "cont_sentence"
  top: "lstm1"
  recurrent_param {
    num_output: 1000
    weight_filler {
      type: "uniform"
      min: -0.08
      max: 0.08
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "lstm1-drop"
  type: "Dropout"
  bottom: "lstm1"
  top: "lstm1"
  dropout_param { dropout_ratio: 0.5 }
  include { stage: "lstm-drop" }
}
layer {
  name: "lstm2"
  type: "LSTM"
  bottom: "lstm1"
  bottom: "cont_sentence"
  top: "lstm2"
  recurrent_param {
    num_output: 1000
    weight_filler {
      type: "uniform"
      min: -0.08
      max: 0.08
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "lstm2-drop"
  type: "Dropout"
  bottom: "lstm2"
  top: "lstm2"
  dropout_param { dropout_ratio: 0.5 }
  include { stage: "lstm-drop" }
}
layer {
  name: "predict"
  type: "InnerProduct"
  bottom: "lstm2"
  top: "predict"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 29
    weight_filler {
      type: "uniform"
      min: -0.08
      max: 0.08
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    axis: 2
  }
}
layer {
  name: "cross_entropy_loss"
  type: "SoftmaxWithLoss"
  bottom: "predict"
  bottom: "target_sentence"
  top: "cross_entropy_loss"
  loss_weight: 20
  loss_param {
    ignore_label: -1
  }
  softmax_param {
    axis: 2
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "predict"
  bottom: "target_sentence"
  top: "accuracy"
  include { phase: TEST }
  accuracy_param {
    axis: 2
    ignore_label: -1
  }
}
</code></pre>
</div></div>

While training, feed names as input and move names one character left as target output, i.e. input S-H-A-N-D-R-I-S, target H-A-N-D-R-I-S-'\n'. The network will learn to predict what the next character will most probably be according to the context above. And if we direct the network's predict output back to the input, the network will loops with its prediction and generate a list of names.

The [result](/ext/nefunique10502.txt) is really amazing. The sheet below shows two columns of names, one is generated by the LSTM network, and another is given by Blizzard staffs. Could you recognize which column is generated by the machine?

<table>
<tr><td>Clathiel</td><td>Clarindrela</td></tr>
<tr><td>Doriana</td><td>Dahlia</td></tr>
<tr><td>Elanndia</td><td>Elessaria</td></tr>
<tr><td>Kulanai</td><td>Kynreith</td></tr>
<tr><td>Lanandris</td><td>Laria</td></tr>
<tr><td>Shyn'tel</td><td>Shauana</td></tr>
<tr><td>Tylaria</td><td>Tarindrella</td></tr>
</table>
